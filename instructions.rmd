These are the instructions:

```{r}
# Load other packages here.
if (!require("tidyverse")) install.packages("tidyverse"); library(tidyverse)

knitr::opts_chunk$set(echo = TRUE)
if (!require("UsingR")) install.packages("UsingR"); library(UsingR)
if (!require("cowplot")) install.packages("cowplot"); library(cowplot)
if (!require("car")) install.packages("car"); library(car) # Anova function for better lm summary stats
if (!require("multcomp")) install.packages("multcomp"); library(multcomp) # glht function for Tukey test
if (!require("emmeans")) install.packages("emmeans"); library(emmeans) # emmeans and pairs functions for post-hoc analysis
if (!require("conflicted")) install.packages("conflicted"); library(conflicted) # For dealing with conflicts
if (!require("tidyverse")) install.packages("tidyverse"); library(tidyverse) # For everything
conflict_prefer_all("dplyr", quiet = TRUE)
library(ggplot2)
```


```{r}

Data <- read.csv("dinosaurus_dozen_clean.csv")
Data |> 
  group_by(dataset) |> 
  summarize(
      mean_x    = mean(x),
      mean_y    = mean(y),
      std_dev_x = sd(x),
      std_dev_y = sd(y),
      corr_x_y  = cor(x, y)
      )

A <- Data |>
  filter(dataset == "a")
B <- Data |>
  filter(dataset == "b")
C <- Data |>
  filter(dataset == "c")
D <- Data |>
  filter(dataset == "d")
E <- Data |>
  filter(dataset == "e")
F <- Data |>
  filter(dataset == "f")
G <- Data |>
  filter(dataset == "g")
H <- Data |>
  filter(dataset == "h")
I <- Data |>
  filter(dataset == "i")
J <- Data |>
  filter(dataset == "j")
K <- Data |>
  filter(dataset == "k")
L <- Data |>
  filter(dataset == "l")
M <- Data |>
  filter(dataset == "m")

```


In this section you have been given one example formula for generating a Wilkes-Shapiro test which is meant to test for normality. You are to run this test on every other dataset to determine which one is your target dataset. The target dataset has a P-value for it's Xs of 0.118 and a W value of 0.98479. for its Ys, 0.0001252 and 0.95449 respectively. What do these values tell you and why?

```{r}
shapiro.test(M$x)
shapiro.test(M$y)
```


Let's try generalized linear modeling, this will tell you the Akaike Information Criterion values. We will also run a Bayesian Information Criterion formula on the same linear model which tests the same things. You have been given one formula as an example, and now you have to run the same for the other datasets. Your target dataset has an AIC of 1383 and a BIC of 1394.847
```{r}
Fglm = glm(X ~ x + y, data=F)
summary(Fglm)
BIC(Fglm)

```



```{r}
Data |>
  ggplot(aes(x = x, colour = dataset)) +
    geom_boxplot() +
    theme_void() +
    theme(legend.position = "none") +
    facet_wrap(~dataset, ncol = 3)
```

```{r}
Data |>
  ggplot(aes(x = y, colour = dataset))+
    geom_boxplot()+
    theme_void()+
    theme(legend.position = "none")+
    facet_wrap(~dataset, ncol = 3)
```

```{r}
Data |> 
  ggplot() +
  aes(x = x, color = dataset, fill = dataset) +
  geom_histogram() +
  facet_wrap(~dataset)+
  theme_cowplot()
```

```{r}
Data |> 
  ggplot() +
  aes(x = y, color = dataset, fill = dataset) +
  geom_histogram() +
  facet_wrap(~dataset)+
  theme_cowplot()
```


```{r}
Data |> 
  ggplot() +
  aes(x = x,  fill = dataset) + 
  geom_density(alpha=.3) +
  facet_wrap(~dataset)
  theme_cowplot()
```

```{r}
Data |> 
  ggplot() +
  aes(x = y,  fill = dataset) + 
  geom_density(alpha=.3) +
  facet_wrap(~dataset)
  theme_cowplot()
```

```{r}
Data |> 
  ggplot(aes(x = x, y = y, colour = dataset))+
    geom_point()+
    theme_void()+
    theme(legend.position = "none")+
    facet_wrap(~dataset)
```

